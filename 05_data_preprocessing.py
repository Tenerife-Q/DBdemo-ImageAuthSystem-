"""
æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç† - ä¸ºæ¨¡å‹è®­ç»ƒåšå‡†å¤‡
ç›®æ ‡ï¼šå°†çˆ¬è™«æ•°æ®è½¬æ¢æˆæ¨¡å‹å¯ç”¨çš„æ ¼å¼
"""

import pandas as pd
import os
import json
from datetime import datetime
import re
from collections import Counter

class DataPreprocessor: 
    """æ•°æ®é¢„å¤„ç†å™¨"""
    
    def __init__(self, csv_file='reddit_posts.csv'):
        self.csv_file = csv_file
        self.df = None
        self.output_dir = 'processed_data'
        os.makedirs(self.output_dir, exist_ok=True)
        print(f"âœ“ åˆå§‹åŒ–å®Œæˆ: {self.output_dir}/")
    
    def load_data(self):
        """åŠ è½½CSVæ•°æ®"""
        print("\n" + "="*70)
        print("ğŸ“‚ ç¬¬1æ­¥ï¼šåŠ è½½æ•°æ®")
        print("="*70)
        
        self.df = pd.read_csv(self.csv_file)
        print(f"âœ… åŠ è½½æˆåŠŸ")
        print(f"   è¡Œæ•°:  {len(self.df)}")
        print(f"   åˆ—æ•°: {len(self.df.columns)}")
        print(f"   åˆ—å: {list(self.df.columns)}")
    
    def clean_text(self, text):
        """æ¸…æ´—æ–‡æœ¬"""
        if pd.isna(text):
            return ""
        
        text = str(text)
        
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦å’Œé“¾æ¥
        text = re.sub(r'http\S+|www\S+', '', text)  # ç§»é™¤URL
        text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # ä¿ç•™åªæœ‰å­—æ¯ã€æ•°å­—ã€ç©ºæ ¼
        
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        text = ' '.  join(text.split())
        
        # è½¬å°å†™
        text = text.lower()
        
        return text
    
    def preprocess_text(self):
        """æ¸…æ´—æ‰€æœ‰æ–‡æœ¬"""
        print("\n" + "="*70)
        print("âœ‚ï¸  ç¬¬2æ­¥ï¼šæ¸…æ´—æ–‡æœ¬æ•°æ®")
        print("="*70)
        
        print("æ­£åœ¨æ¸…æ´—æ ‡é¢˜...")
        self.df['title_clean'] = self.df['title'].apply(self.clean_text)
        
        print("æ­£åœ¨æ¸…æ´—æ–‡æœ¬å†…å®¹...")
        self.df['text_clean'] = self.df['text'].apply(self.clean_text)
        
        # è®¡ç®—æ–‡æœ¬é•¿åº¦
        self.df['title_length'] = self.df['title_clean'].str.len()
        self.df['text_length'] = self. df['text_clean'].  str.len()
        
        print(f"âœ… æ¸…æ´—å®Œæˆ")
        print(f"   å¹³å‡æ ‡é¢˜é•¿åº¦: {self.df['title_length'].mean():.0f} å­—")
        print(f"   å¹³å‡æ–‡æœ¬é•¿åº¦: {self.df['text_length'].mean():.0f} å­—")
    
    def create_labels(self):
        """åˆ›å»ºæ ‡ç­¾ï¼ˆçœŸ/å‡åˆ†ç±»ï¼‰"""
        print("\n" + "="*70)
        print("ğŸ·ï¸  ç¬¬3æ­¥ï¼šåˆ›å»ºæ ‡ç­¾")
        print("="*70)
        
        print("åŸºäºæ•°æ®æºåˆ›å»ºæ ‡ç­¾...")
        
        # ç®€å•çš„æ ‡ç­¾è§„åˆ™ï¼ˆå®é™…é¡¹ç›®ä¼šæ›´å¤æ‚ï¼‰
        def assign_label(row):
            # nottheonion æ˜¯"ä¸æ˜¯æ´‹è‘±æ–°é—»"ï¼Œé€šå¸¸æ˜¯çœŸå®ä½†è’è°¬çš„æ–°é—»
            # news å’Œ worldnews ä¸€èˆ¬æ˜¯çœŸå®æ–°é—»
            # æˆ‘ä»¬è¿™é‡Œç®€åŒ–å¤„ç†
            
            subreddit = row['subreddit']. lower()
            
            if subreddit == 'nottheonion': 
                return 1  # çœŸå®ï¼ˆä½†éœ€è¦éªŒè¯ï¼‰
            elif subreddit in ['news', 'worldnews']:
                return 1  # çœŸå®æ–°é—»
            else:
                return 0  # æœªçŸ¥
        
        self.df['label'] = self.df. apply(assign_label, axis=1)
        
        # æ ‡ç­¾ç»Ÿè®¡
        label_counts = self.df['label'].value_counts()
        print(f"âœ… æ ‡ç­¾åˆ›å»ºå®Œæˆ")
        print(f"   çœŸå®æ–°é—»: {label_counts. get(1, 0)} ä¸ª")
        print(f"   å…¶ä»–:  {label_counts.get(0, 0)} ä¸ª")
    
    def filter_valid_data(self):
        """è¿‡æ»¤æœ‰æ•ˆæ•°æ®"""
        print("\n" + "="*70)
        print("ğŸ” ç¬¬4æ­¥ï¼šè¿‡æ»¤æœ‰æ•ˆæ•°æ®")
        print("="*70)
        
        print("è¿‡æ»¤å‰:")
        print(f"  æ€»æ•°: {len(self.df)}")
        
        # è¿‡æ»¤æ‰æ²¡æœ‰æ–‡æœ¬æˆ–æ ‡é¢˜çš„
        self.df = self.df[self.df['title_length'] > 5]
        
        print("è¿‡æ»¤å:")
        print(f"  æ€»æ•°: {len(self.df)}")
        print(f"âœ… è¿‡æ»¤å®Œæˆ")
    
    def analyze_cleaned_data(self):
        """åˆ†ææ¸…æ´—åçš„æ•°æ®"""
        print("\n" + "="*70)
        print("ğŸ“Š ç¬¬5æ­¥ï¼šæ•°æ®åˆ†æ")
        print("="*70)
        
        print(f"\næ–‡æœ¬ç»Ÿè®¡:")
        print(f"  æ ‡é¢˜å¹³å‡é•¿åº¦: {self.df['title_length'].mean():.0f} å­—")
        print(f"  æ–‡æœ¬å¹³å‡é•¿åº¦: {self.df['text_length'].mean():.0f} å­—")
        print(f"  æœ€é•¿æ ‡é¢˜: {self.df['title_length'].max()} å­—")
        print(f"  æœ€é•¿æ–‡æœ¬:  {self.df['text_length'].max()} å­—")
        
        print(f"\nèµæ•°ç»Ÿè®¡:")
        print(f"  å¹³å‡èµæ•°: {self.df['score'].mean():.0f}")
        print(f"  ä¸­ä½èµæ•°: {self.df['score'].median():.0f}")
        print(f"  æœ€é«˜èµæ•°: {self. df['score'].max()}")
        
        print(f"\nå›¾ç‰‡åˆ†å¸ƒ:")
        has_image = len(self.df[self.df['image_url'].notna() & (self.df['image_url']. str.len() > 0)])
        print(f"  æœ‰å›¾ç‰‡: {has_image} ä¸ª ({has_image/len(self.df)*100:.1f}%)")
        print(f"  æ— å›¾ç‰‡: {len(self. df) - has_image} ä¸ª ({(len(self.df)-has_image)/len(self.df)*100:.1f}%)")
        
        print(f"\nå…³é”®è¯åˆ†æ (æ ‡é¢˜ä¸­æœ€å¸¸è§çš„è¯):")
        all_words = ' '.join(self.df['title_clean']).split()
        word_counts = Counter(all_words)
        
        # ç§»é™¤å¸¸ç”¨è¯
        stopwords = {'the', 'a', 'and', 'or', 'of', 'in', 'to', 'is', 'that', 'for', 'on'}
        common_words = [word for word, count in word_counts.most_common(10) if word not in stopwords]
        
        for i, word in enumerate(common_words[: 5], 1):
            count = word_counts[word]
            print(f"  {i}. {word}:  {count} æ¬¡")
    
    def save_processed_data(self):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        print("\n" + "="*70)
        print("ğŸ’¾ ç¬¬6æ­¥ï¼šä¿å­˜å¤„ç†åçš„æ•°æ®")
        print("="*70)
        
        # ä¿å­˜ä¸ºCSV
        csv_path = os.path.join(self.output_dir, 'reddit_posts_cleaned.csv')
        self.df.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"âœ… CSVå·²ä¿å­˜:  {csv_path}")
        
        # ä¿å­˜ä¸ºJSONï¼ˆåŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼‰
        json_path = os.path.join(self. output_dir, 'reddit_posts_cleaned.json')
        self.df.to_json(json_path, orient='records', force_ascii=False, indent=2)
        print(f"âœ… JSONå·²ä¿å­˜: {json_path}")
        
        # åˆ›å»ºç»Ÿè®¡æŠ¥å‘Š
        report_path = os.path.join(self.output_dir, 'data_report.txt')
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write("="*70 + "\n")
            f.write("Redditæ•°æ®é¢„å¤„ç†æŠ¥å‘Š\n")
            f.write("="*70 + "\n\n")
            
            f.write(f"å¤„ç†æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f. write("æ•°æ®ç»Ÿè®¡:\n")
            f.write(f"  æ€»æ ·æœ¬æ•°: {len(self. df)}\n")
            f.write(f"  çœŸå®æ–°é—»: {len(self.df[self.df['label']==1])}\n")
            f.write(f"  å…¶ä»–: {len(self.df[self.df['label']==0])}\n\n")
            
            f.write("æ–‡æœ¬ç»Ÿè®¡:\n")
            f.write(f"  æ ‡é¢˜å¹³å‡é•¿åº¦: {self.df['title_length'].mean():.0f} å­—\n")
            f.write(f"  æ–‡æœ¬å¹³å‡é•¿åº¦: {self.df['text_length'].mean():.0f} å­—\n\n")
            
            f. write("ç¤¾äº¤æŒ‡æ ‡:\n")
            f.write(f"  å¹³å‡èµæ•°: {self. df['score'].mean():.0f}\n")
            f.write(f"  å¹³å‡è¯„è®º:  {self.df['comments'].mean():.0f}\n\n")
            
            f. write("æ•°æ®è´¨é‡:\n")
            f.write(f"  åŒ…å«å›¾ç‰‡: {len(self.df[self.df['image_url'].notna()]) / len(self.df) * 100:.1f}%\n")
            f.write(f"  æœ‰æ•ˆè¡Œ: {len(self.df)}\n")
        
        print(f"âœ… æŠ¥å‘Šå·²ä¿å­˜: {report_path}")
    
    def display_samples(self, num_samples=3):
        """æ˜¾ç¤ºæ¸…æ´—åçš„æ ·æœ¬"""
        print("\n" + "="*70)
        print(f"ğŸ“‹ æ ·æœ¬æ•°æ® (å‰{num_samples}ä¸ª)")
        print("="*70)
        
        for idx, row in self.df.head(num_samples).iterrows():
            print(f"\n[æ ·æœ¬ {idx+1}]")
            print(f"  æ ‡é¢˜: {row['title'][:   60]}")
            print(f"  æ¸…æ´—å: {row['title_clean'][:  60]}")
            print(f"  æ ‡ç­¾: {'çœŸå®' if row['label'] == 1 else 'å…¶ä»–'} ({row['label']})")
            print(f"  èµ:  {row['score']} | è¯„è®º: {row['comments']}")
            print(f"  æ–‡æœ¬é•¿åº¦: {row['text_length']}")
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´çš„é¢„å¤„ç†æµç¨‹"""
        print("\nğŸš€ å¼€å§‹æ•°æ®é¢„å¤„ç†.. .\n")
        
        self.load_data()
        self.preprocess_text()
        self.create_labels()
        self.filter_valid_data()
        self.analyze_cleaned_data()
        self.display_samples()
        self.save_processed_data()
        
        print("\n" + "="*70)
        print("âœ… æ•°æ®é¢„å¤„ç†å®Œæˆ!")
        print("="*70)
        print(f"\nè¾“å‡ºæ–‡ä»¶ä½ç½®: {self.output_dir}/")
        print(f"  â”œâ”€â”€ reddit_posts_cleaned.csv  (å¯ç”¨Excelæ‰“å¼€)")
        print(f"  â”œâ”€â”€ reddit_posts_cleaned.json (JSONæ ¼å¼)")
        print(f"  â””â”€â”€ data_report. txt (ç»Ÿè®¡æŠ¥å‘Š)")

# ==================== è¿è¡Œ ====================
if __name__ == "__main__":
    preprocessor = DataPreprocessor('reddit_posts.csv')
    preprocessor.run()