"""
åŠ è½½Redditçˆ¬è™«æ•°æ® - ä¸ºåç»­æ¨¡å‹è®­ç»ƒåšå‡†å¤‡
"""

import json
import os
import pandas as pd
from datetime import datetime

class DataLoader:
    """æ•°æ®åŠ è½½å™¨"""
    
    def __init__(self, data_dir='reddit_data'):
        self.data_dir = data_dir
    
    def load_json_files(self):
        """åŠ è½½æ‰€æœ‰JSONæ–‡ä»¶"""
        print("="*70)
        print("ğŸ“‚ åŠ è½½Redditæ•°æ®")
        print("="*70)
        
        all_posts = []
        
        # éå†data_dirä¸­çš„æ‰€æœ‰JSONæ–‡ä»¶
        for filename in os.listdir(self.data_dir):
            if filename.endswith('.json'):
                filepath = os.path. join(self.data_dir, filename)
                print(f"\nğŸ“– è¯»å–: {filename}")
                
                with open(filepath, 'r', encoding='utf-8') as f:
                    posts = json.load(f)
                    all_posts.extend(posts)
                    print(f"   âœ“ åŠ è½½ {len(posts)} ä¸ªå¸–å­")
        
        print(f"\nâœ… æ€»å…±åŠ è½½:  {len(all_posts)} ä¸ªå¸–å­")
        return all_posts
    
    def convert_to_dataframe(self, posts):
        """è½¬æ¢ä¸ºPandas DataFrameï¼ˆä¾¿äºåˆ†æï¼‰"""
        print("\n" + "="*70)
        print("ğŸ“Š è½¬æ¢ä¸ºæ•°æ®è¡¨")
        print("="*70)
        
        df = pd.DataFrame(posts)
        
        print(f"\næ•°æ®è¡¨ä¿¡æ¯:")
        print(f"  è¡Œæ•°: {len(df)}")
        print(f"  åˆ—æ•°: {len(df.columns)}")
        print(f"  åˆ—å: {list(df.columns)}")
        
        return df
    
    def analyze_data(self, df):
        """åˆ†ææ•°æ®"""
        print("\n" + "="*70)
        print("ğŸ“ˆ æ•°æ®åˆ†æ")
        print("="*70)
        
        print(f"\nåŸºç¡€ç»Ÿè®¡:")
        print(f"  å¹³å‡èµæ•°: {df['score']. mean():.1f}")
        print(f"  å¹³å‡è¯„è®º:  {df['comments'].mean():.1f}")
        print(f"  æœ€é«˜èµæ•°: {df['score'].max()}")
        print(f"  æœ€ä½èµæ•°: {df['score'].min()}")
        
        print(f"\næ–‡æœ¬åˆ†æ:")
        print(f"  æœ‰æ–‡æœ¬çš„å¸–å­: {len(df[df['text'].str.len() > 0])} ä¸ª")
        print(f"  æœ‰å›¾ç‰‡çš„å¸–å­: {len(df[df['image_url'].str.len() > 0])} ä¸ª")
        print(f"  å¹³å‡æ–‡æœ¬é•¿åº¦: {df['text'].str.len().mean():.0f} å­—")
        
        print(f"\nå­ç‰ˆå—åˆ†å¸ƒ:")
        print(df['subreddit'].value_counts())
        
        return df
    
    def save_to_csv(self, df, output_file='reddit_posts.csv'):
        """ä¿å­˜ä¸ºCSVï¼ˆä¾¿äºExcelæ‰“å¼€ï¼‰"""
        print(f"\nğŸ’¾ ä¿å­˜ä¸ºCSV: {output_file}")
        df.to_csv(output_file, index=False, encoding='utf-8-sig')
        print(f"âœ… å·²ä¿å­˜")
    
    def display_samples(self, df, num_samples=3):
        """æ˜¾ç¤ºæ ·æœ¬æ•°æ®"""
        print("\n" + "="*70)
        print(f"ğŸ“‹ æ•°æ®æ ·æœ¬ (å‰{num_samples}ä¸ª)")
        print("="*70)
        
        for idx, row in df.head(num_samples).iterrows():
            print(f"\n[{idx+1}]")
            print(f"  æ ‡é¢˜: {row['title'][:  60]}")
            print(f"  ä½œè€…: {row['author']}")
            print(f"  èµ:  {row['score']} | è¯„è®º: {row['comments']}")
            print(f"  æ–‡æœ¬: {row['text'][: 80]}...")
            print(f"  å›¾ç‰‡: {'æœ‰' if row['image_url'] else 'æ— '}")

# ==================== è¿è¡Œ ====================
if __name__ == "__main__": 
    loader = DataLoader('reddit_data')
    
    # 1. åŠ è½½JSONæ–‡ä»¶
    posts = loader.load_json_files()
    
    # 2. è½¬æ¢ä¸ºDataFrame
    df = loader.convert_to_dataframe(posts)
    
    # 3. åˆ†ææ•°æ®
    df = loader.analyze_data(df)
    
    # 4. æ˜¾ç¤ºæ ·æœ¬
    loader.display_samples(df, num_samples=3)
    
    # 5. ä¿å­˜ä¸ºCSV
    loader.save_to_csv(df, 'reddit_posts.csv')
    
    print("\n" + "="*70)
    print("âœ… æ•°æ®åŠ è½½å®Œæˆ!")
    print("="*70)
    print("\nå¯ä»¥ç”¨Excelæ‰“å¼€ reddit_posts.csv æŸ¥çœ‹æ•°æ®")