"""
Redditçˆ¬è™« - çˆ¬å–Redditä¸Šçš„æ–°é—»å’Œå›¾ç‰‡
é€‚åˆè™šå‡ä¿¡æ¯æ£€æµ‹é¡¹ç›®
"""

import requests
import json
import os
from datetime import datetime
import time

class RedditScraper: 
    """Redditçˆ¬è™« - çˆ¬å–å­ç‰ˆå—æ•°æ®"""
    
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    def __init__(self, output_dir='reddit_data'):
        self.output_dir = output_dir
        os.makedirs(output_dir, exist_ok=True)
        print(f"âœ“ åˆå§‹åŒ–å®Œæˆ: {output_dir}/")
    
    def scrape_subreddit(self, subreddit_name, post_count=30):
        """
        çˆ¬å–Redditå­ç‰ˆå—
        
        å‚æ•°:
        - subreddit_name: ç‰ˆå—åç§° (å¦‚ 'news', 'worldnews')
        - post_count: è¦çˆ¬å–çš„å¸–å­æ•°
        """
        print("\n" + "="*70)
        print(f"ğŸ” çˆ¬å– Reddit - r/{subreddit_name}")
        print("="*70)
        
        try:
            # Redditå®˜æ–¹JSON API
            url = f"https://www.reddit.com/r/{subreddit_name}/new.json"
            
            print(f"\n[1/3] è¿æ¥åˆ°:  {url}")
            
            # å‘é€è¯·æ±‚
            response = requests.get(
                url,
                headers=self.HEADERS,
                params={'limit': post_count},
                timeout=10
            )
            
            if response.status_code != 200:
                print(f"âŒ å¤±è´¥!  çŠ¶æ€ç : {response. status_code}")
                return None
            
            print("âœ… è¿æ¥æˆåŠŸ")
            
            # è§£æJSONæ•°æ®
            print(f"\n[2/3] è§£ææ•°æ®...")
            data = response. json()
            posts = []
            
            # æå–å¸–å­ä¿¡æ¯
            for post_data in data['data']['children']:
                post = post_data['data']
                
                # è·å–å…³é”®ä¿¡æ¯
                post_info = {
                    'id': post. get('id', ''),
                    'title': post.get('title', ''),
                    'text': post.get('selftext', '')[:  300],  # å‰300å­—
                    'author':  post.get('author', ''),
                    'subreddit': post.get('subreddit', ''),
                    'score': post.get('score', 0),  # èµæ•°
                    'comments': post.get('num_comments', 0),  # è¯„è®ºæ•°
                    'url': post.get('url', ''),
                    'image_url': post.get('preview', {}).get('images', [{}])[0].get('source', {}).get('url', ''),
                    'created_at': datetime.fromtimestamp(post.get('created_utc', 0)).isoformat()
                }
                
                posts.append(post_info)
            
            print(f"âœ… è§£æå®Œæˆï¼Œæ‰¾åˆ° {len(posts)} ä¸ªå¸–å­")
            
            # ä¿å­˜ä¸ºJSON
            print(f"\n[3/3] ä¿å­˜æ•°æ®...")
            filename = os.path.join(
                self.output_dir,
                f"r_{subreddit_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            )
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(posts, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æ•°æ®å·²ä¿å­˜:  {filename}")
            
            # ç»Ÿè®¡ä¿¡æ¯
            print("\n" + "="*70)
            print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯ (r/{subreddit_name}):")
            print("="*70)
            
            total_score = sum(p['score'] for p in posts)
            total_comments = sum(p['comments'] for p in posts)
            posts_with_images = len([p for p in posts if p['image_url']])
            posts_with_text = len([p for p in posts if p['text']])
            
            print(f"  ğŸ“ æ€»å¸–å­æ•°: {len(posts)}")
            print(f"  ğŸ‘ æ€»èµæ•°: {total_score}")
            print(f"  ğŸ’¬ æ€»è¯„è®ºæ•°: {total_comments}")
            print(f"  ğŸ–¼ï¸  åŒ…å«å›¾ç‰‡: {posts_with_images} ä¸ª")
            print(f"  ğŸ“„ åŒ…å«æ–‡æœ¬: {posts_with_text} ä¸ª")
            print(f"  â­ å¹³å‡è¯„åˆ†: {total_score / len(posts) if posts else 0:.1f}")
            
            # æ˜¾ç¤ºå‰3ä¸ªå¸–å­
            print(f"\nå‰3ä¸ªçƒ­é—¨å¸–å­:")
            print("-"*70)
            for i, post in enumerate(sorted(posts, key=lambda x:  x['score'], reverse=True)[:  3], 1):
                print(f"\n{i}. [{post['score']} èµ] {post['title'][:  60]}")
                print(f"   ä½œè€…: {post['author']} | è¯„è®º: {post['comments']}")
            
            print("\n" + "="*70)
            
            return posts
            
        except Exception as e:
            print(f"\nâŒ é”™è¯¯:  {e}")
            return None

# ==================== è¿è¡Œ ====================
if __name__ == "__main__":
    scraper = RedditScraper('reddit_data')
    
    # çˆ¬å–å¤šä¸ªå­ç‰ˆå—
    subreddits = [
        'news',          # æ–°é—»
        'worldnews',     # ä¸–ç•Œæ–°é—»  
        'nottheonion',   # ä¸æ˜¯æ´‹è‘±æ–°é—»ï¼ˆçœŸå®ä½†è’è°¬çš„æ–°é—»ï¼‰
    ]
    
    print("\nğŸš€ å¼€å§‹çˆ¬å–Redditæ•°æ®...")
    print(f"å°†çˆ¬å– {len(subreddits)} ä¸ªå­ç‰ˆå—\n")
    
    for subreddit in subreddits: 
        scraper.scrape_subreddit(subreddit, post_count=20)
        time.sleep(2)  # ç¤¼è²Œå»¶è¿Ÿï¼ˆé¿å…è¢«Banï¼‰
    
    print("\nâœ… æ‰€æœ‰çˆ¬è™«ä»»åŠ¡å®Œæˆ!")
    print("ğŸ“ æŸ¥çœ‹ reddit_data/ æ–‡ä»¶å¤¹æŸ¥çœ‹JSONæ•°æ®")