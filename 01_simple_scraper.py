"""
ğŸ“ æˆ‘çš„ç¬¬ä¸€ä¸ªçˆ¬è™« - Wikipediaå›¾æ–‡çˆ¬å–
è¿™æ˜¯æœ€ç®€å•çš„ç‰ˆæœ¬ï¼Œç”¨æ¥ç†è§£åŸºç¡€åŸç†
"""

import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin
from PIL import Image
from io import BytesIO
import time

# ==================== ç¬¬ä¸€éƒ¨åˆ†ï¼šé…ç½® ====================
class Config:
    """çˆ¬è™«é…ç½® - æ”¹è¿™é‡Œå°±èƒ½æ”¹çˆ¬è™«è¡Œä¸º"""
    
    # ç›®æ ‡ç½‘å€ï¼ˆWikipediaè¯æ¡ï¼‰
    TARGET_URL = "https://en.wikipedia.org/wiki/Misinformation"
    
    # ä¿å­˜æ–‡ä»¶å¤¹
    OUTPUT_DIR = "downloaded_data"
    
    # å›¾ç‰‡æ–‡ä»¶å¤¹
    IMAGES_DIR = os.path.join(OUTPUT_DIR, "images")
    
    # è¦ä¸‹è½½çš„å›¾ç‰‡æ•°é‡
    MAX_IMAGES = 10
    
    # è¯·æ±‚å¤´ï¼ˆå‘Šè¯‰æœåŠ¡å™¨ä½ æ˜¯æµè§ˆå™¨ï¼‰
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }

# ==================== ç¬¬äºŒéƒ¨åˆ†ï¼šçˆ¬è™«ä¸»é€»è¾‘ ====================
class WikipediaScraper: 
    """Wikipediaçˆ¬è™« - ä»ç»´åŸºç™¾ç§‘çˆ¬å–å›¾æ–‡æ•°æ®"""
    
    def __init__(self, config=Config):
        self.config = config
        # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåŒæ—¶åˆ›å»º images å­ç›®å½•ï¼‰
        os.makedirs(self.config.OUTPUT_DIR, exist_ok=True)
        os.makedirs(self.config.IMAGES_DIR, exist_ok=True)
        print(f"âœ“ åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.config.OUTPUT_DIR}")
    
    def step1_download_html(self, url):
        """
        ç¬¬ä¸€æ­¥ï¼šä¸‹è½½ç½‘é¡µ
        ä½œç”¨ï¼šè·å–ç½‘é¡µçš„HTMLä»£ç 
        """
        print("\n" + "="*60)
        print("ğŸ“– ç¬¬ä¸€æ­¥ï¼šä¸‹è½½ç½‘é¡µ")
        print("="*60)
        
        try: 
            print(f"æ­£åœ¨è®¿é—®: {url}")
            
            # å…³é”®ä»£ç ï¼šå‘é€HTTPè¯·æ±‚
            response = requests.get(url, headers=self.config.HEADERS, timeout=10)
            
            # æ£€æŸ¥æ˜¯å¦æˆåŠŸ
            if response. status_code == 200:
                print(f"âœ… æˆåŠŸï¼çŠ¶æ€ç : {response.status_code}")
                print(f"   ç½‘é¡µå¤§å°: {len(response.text)/1024:.1f} KB")
                return response
            else:
                print(f"âŒ å¤±è´¥ï¼çŠ¶æ€ç : {response.status_code}")
                return None
                
        except Exception as e:
            print(f"âŒ é”™è¯¯:  {e}")
            return None
    
    def step2_parse_html(self, response):
        """
        ç¬¬äºŒæ­¥ï¼šè§£æHTML
        ä½œç”¨ï¼šä»HTMLä¸­æå–æˆ‘ä»¬éœ€è¦çš„ä¿¡æ¯
        """
        print("\n" + "="*60)
        print("ğŸ” ç¬¬äºŒæ­¥ï¼šè§£æHTML")
        print("="*60)
        
        try:
            # å…³é”®ä»£ç ï¼šä½¿ç”¨BeautifulSoupè§£æ
            soup = BeautifulSoup(response.text, 'html.parser')
            print("âœ… HTMLè§£ææˆåŠŸ")
            
            # æå–é¡µé¢æ ‡é¢˜
            title = soup.find('h1', class_='firstHeading')
            if title:
                print(f"   é¡µé¢æ ‡é¢˜: {title.get_text()}")
            
            # æå–é¡µé¢æ‘˜è¦
            summary = soup. find('p')
            if summary:
                summary_text = summary.get_text()[:100]
                print(f"   é¡µé¢æ‘˜è¦: {summary_text}...")
            
            return soup
            
        except Exception as e:
            print(f"âŒ è§£æå¤±è´¥: {e}")
            return None
    
    def step3_extract_images(self, soup):
        """
        ç¬¬ä¸‰æ­¥ï¼šæå–å›¾ç‰‡
        ä½œç”¨ï¼šä»HTMLä¸­æ‰¾å‡ºæ‰€æœ‰å›¾ç‰‡çš„é“¾æ¥
        """
        print("\n" + "="*60)
        print("ğŸ–¼ï¸  ç¬¬ä¸‰æ­¥ï¼šæå–å›¾ç‰‡é“¾æ¥")
        print("="*60)
        
        try:  
            # æ–¹æ³•1ï¼šæ‰¾æ‰€æœ‰imgæ ‡ç­¾
            all_images = soup.find_all('img')
            print(f"âœ… æ‰¾åˆ° {len(all_images)} ä¸ªå›¾ç‰‡æ ‡ç­¾")
            
            # æ–¹æ³•2ï¼šè¿‡æ»¤å‡ºæœ‰ç”¨çš„å›¾ç‰‡ï¼ˆè¿‡æ»¤æ‰logoã€iconç­‰ï¼‰
            useful_images = []
            for idx, img in enumerate(all_images):
                src = img.get('src')
                alt = img.get('alt', '')
                
                # åªè¦æœ‰srcå±æ€§çš„å›¾ç‰‡
                if src:  
                    useful_images.append({
                        'src': src,
                        'alt': alt,
                        'idx': idx
                    })
            
            print(f"   æœ‰æ•ˆå›¾ç‰‡:  {len(useful_images)} ä¸ª")
            
            # æ˜¾ç¤ºå‰5ä¸ª
            print("\n   å‰5ä¸ªå›¾ç‰‡:")
            for i, img in enumerate(useful_images[: 5]):
                print(f"      {i+1}. {img['alt'][: 50]}")
            
            return useful_images
            
        except Exception as e:
            print(f"âŒ æå–å¤±è´¥: {e}")
            return []
    
    def step4_download_images(self, response, image_list):
        """
        ç¬¬å››æ­¥ï¼šä¸‹è½½å›¾ç‰‡
        ä½œç”¨ï¼šå°†å›¾ç‰‡URLè½¬æ¢æˆæœ¬åœ°æ–‡ä»¶
        """
        print("\n" + "="*60)
        print("â¬‡ï¸  ç¬¬å››æ­¥ï¼šä¸‹è½½å›¾ç‰‡")
        print("="*60)
        
        downloaded_count = 0
        failed_count = 0
        
        for idx, img_info in enumerate(image_list[: self.config.MAX_IMAGES]):
            src = img_info['src'] or ''
            
            # è·³è¿‡ data URI
            if src.startswith('data:'):
                print("   â­ï¸  è·³è¿‡ data URL")
                continue
            
            # ä½¿ç”¨ urljoin è§„èŒƒåŒ– URLï¼ˆå¤„ç† //ã€/ å’Œç›¸å¯¹è·¯å¾„ï¼‰
            src = urljoin(response.url, src)
            
            try:
                print(f"\n   [{idx+1}/{min(self.config.MAX_IMAGES, len(image_list))}] ä¸‹è½½ä¸­...")
                print(f"   URL: {src[: 120]}...")
                
                # ä¸‹è½½å›¾ç‰‡
                img_response = requests.get(src, timeout=10, headers=self.config.HEADERS)
                
                if img_response.status_code == 200:
                    # å°è¯•ç”¨ PIL æ‰“å¼€å›¾ç‰‡ï¼Œå¤±è´¥åˆ™è·³è¿‡
                    try:
                        img = Image.open(BytesIO(img_response.content))
                    except Exception as e:
                        print(f"   âŒ æ— æ³•è¯†åˆ«å›¾ç‰‡æ–‡ä»¶: {e}")
                        failed_count += 1
                        continue
                    
                    # è·å–å›¾ç‰‡ä¿¡æ¯
                    width, height = img.size
                    file_format = img.format
                    
                    # åªä¿å­˜è¾ƒå¤§çš„å›¾ç‰‡ï¼ˆè¿‡æ»¤æ‰ iconï¼‰
                    if width > 100 and height > 100:
                        # ä¿®æ­£æ–‡ä»¶åç©ºæ ¼é—®é¢˜ï¼Œç»Ÿä¸€ä¸º .jpg
                        filename = f"image_{idx:03d}.jpg"
                        filepath = os.path.join(self.config.IMAGES_DIR, filename)
                        
                        # è½¬æ¢ä¸ºRGBï¼ˆæœ‰äº›å›¾ç‰‡æ˜¯PNGæˆ–å…¶ä»–æ ¼å¼ï¼‰
                        if img.mode != 'RGB':
                            img = img.convert('RGB')
                        
                        # ä¿å­˜ï¼ˆä»¥ JPEG å½¢å¼ï¼‰
                        try:
                            img.save(filepath, format='JPEG', quality=85)
                        except Exception as e:
                            print(f"   âŒ ä¿å­˜å¤±è´¥: {e}")
                            failed_count += 1
                            continue
                        
                        downloaded_count += 1
                        print(f"   âœ… æˆåŠŸ!  å¤§å°: {width}x{height} æ ¼å¼: {file_format}")
                    else:
                        print(f"   â­ï¸  è·³è¿‡ï¼ˆå¤ªå°:  {width}x{height}ï¼‰")
                else:
                    print(f"   âŒ å›¾ç‰‡è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {img_response.status_code}")
                    failed_count += 1
                    
            except Exception as e:
                failed_count += 1
                print(f"   âŒ å¤±è´¥: {str(e)[:200]}")
        
        print(f"\n   æ€»ç»“:  {downloaded_count} æˆåŠŸ, {failed_count} å¤±è´¥")
        return downloaded_count
    
    def step5_save_metadata(self, soup):
        """
        ç¬¬äº”æ­¥ï¼šä¿å­˜å…ƒæ•°æ®ï¼ˆæ–‡æœ¬ï¼‰
        ä½œç”¨ï¼šä¿å­˜ç½‘é¡µæ–‡æœ¬å†…å®¹å’Œå›¾ç‰‡å¯¹åº”å…³ç³»
        """
        print("\n" + "="*60)
        print("ğŸ“ ç¬¬äº”æ­¥ï¼šä¿å­˜å…ƒæ•°æ®")
        print("="*60)
        
        try:
            # æå–æ‰€æœ‰æ–‡æœ¬
            paragraphs = soup.find_all('p')
            text_content = []
            
            for p in paragraphs[: 10]:  # åªå–å‰10æ®µ
                text = p.get_text().strip()
                if text and len(text) > 20:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„
                    text_content.append(text)
            
            # ä¿å­˜ä¸ºæ–‡æœ¬æ–‡ä»¶
            metadata_path = os.path.join(self.config.OUTPUT_DIR, "metadata.txt")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                f.write("=" * 60 + "\n")
                f.write("çˆ¬è™«æ•°æ®æ±‡æ€»\n")
                f.write("=" * 60 + "\n\n")
                
                f.write(f"æ•°æ®æº: {self.config.TARGET_URL}\n")
                f.write(f"çˆ¬å–æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                f.write("æå–çš„æ–‡æœ¬å†…å®¹:\n")
                f.write("-" * 60 + "\n")
                for i, text in enumerate(text_content, 1):
                    f.write(f"\n{i}. {text[: 200]}...\n")
            
            print(f"âœ… å…ƒæ•°æ®å·²ä¿å­˜:  {metadata_path}")
            print(f"   åŒ…å« {len(text_content)} æ®µæ–‡æœ¬")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜å¤±è´¥: {e}")

# ==================== ç¬¬ä¸‰éƒ¨åˆ†ï¼šä¸»ç¨‹åº ====================
def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œå®Œæ•´çš„çˆ¬è™«æµç¨‹"""
    
    print("\n" + "ğŸ“ "*20)
    print("æ¬¢è¿ä½¿ç”¨ï¼šWikipedia å›¾æ–‡çˆ¬è™«")
    print("ğŸ“ "*20 + "\n")
    
    # 1. åˆ›å»ºçˆ¬è™«å®ä¾‹
    scraper = WikipediaScraper()
    
    # 2. æ‰§è¡Œç¬¬ä¸€æ­¥ï¼šä¸‹è½½HTML
    response = scraper.step1_download_html(Config. TARGET_URL)
    if not response:
        print("âŒ ç¨‹åºä¸­æ­¢")
        return
    
    # 3. æ‰§è¡Œç¬¬äºŒæ­¥ï¼šè§£æHTML
    soup = scraper.step2_parse_html(response)
    if not soup:
        print("âŒ ç¨‹åºä¸­æ­¢")
        return
    
    # 4. æ‰§è¡Œç¬¬ä¸‰æ­¥ï¼šæå–å›¾ç‰‡
    images = scraper.step3_extract_images(soup)
    if not images:
        print("âŒ æ²¡æ‰¾åˆ°å›¾ç‰‡")
        return
    
    # 5. æ‰§è¡Œç¬¬å››æ­¥ï¼šä¸‹è½½å›¾ç‰‡
    downloaded = scraper.step4_download_images(response, images)
    
    # 6. æ‰§è¡Œç¬¬äº”æ­¥ï¼šä¿å­˜å…ƒæ•°æ®
    scraper.step5_save_metadata(soup)
    
    # 7. æœ€ç»ˆæ€»ç»“
    print("\n" + "="*60)
    print("âœ… çˆ¬è™«å®Œæˆ!")
    print("="*60)
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {Config.OUTPUT_DIR}/")
    print(f"   â”œâ”€â”€ images/ ({downloaded} å¼ å›¾ç‰‡)")
    print(f"   â””â”€â”€ metadata.txt (æ–‡æœ¬æ•°æ®)")
    print("\nä¸‹ä¸€æ­¥ï¼šæ£€æŸ¥ downloaded_data æ–‡ä»¶å¤¹æŸ¥çœ‹ç»“æœ ğŸ‘‰")

if __name__ == "__main__":
    main()