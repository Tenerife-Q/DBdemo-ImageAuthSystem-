"""
å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹æ¨¡å‹
èåˆï¼šBERTï¼ˆæ–‡æœ¬ï¼‰ + ResNet50ï¼ˆå›¾ç‰‡ï¼‰
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch. utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import os
from datetime import datetime
import json

print("="*70)
print("ğŸ¤– å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹æ¨¡å‹")
print("="*70)

# ==================== ç¬¬1éƒ¨åˆ†ï¼šæ£€æŸ¥GPU ====================
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nâœ“ ä½¿ç”¨è®¾å¤‡:  {device}")
if device.type == 'cuda': 
    print(f"  GPU:  {torch.cuda.get_device_name(0)}")

# ==================== ç¬¬2éƒ¨åˆ†ï¼šæ•°æ®é›†ç±» ====================
class TextImageDataset(Dataset):
    """æ–‡æœ¬+å›¾ç‰‡æ•°æ®é›†"""
    
    def __init__(self, csv_file, max_samples=None):
        """
        å‚æ•°: 
        - csv_file: CSVæ–‡ä»¶è·¯å¾„
        - max_samples: é™åˆ¶æ ·æœ¬æ•°ï¼ˆæµ‹è¯•ç”¨ï¼‰
        """
        self. df = pd.read_csv(csv_file)
        
        if max_samples:
            self.df = self.df. head(max_samples)
        
        print(f"\nâœ“ åŠ è½½æ•°æ®é›†: {csv_file}")
        print(f"  æ ·æœ¬æ•°: {len(self. df)}")
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        row = self.df.iloc[idx]
        
        # æ–‡æœ¬ç‰¹å¾ï¼ˆç®€åŒ–ç‰ˆï¼šåªç”¨é•¿åº¦å’Œè¯æ•°ï¼‰
        title_len = float(row. get('title_length', 0))
        text_len = float(row.get('text_length', 0))
        score = float(row.get('score', 0))
        comments = float(row.get('comments', 0))
        
        # ç»„åˆæˆæ–‡æœ¬ç‰¹å¾å‘é‡ï¼ˆ4ç»´ï¼‰
        text_features = torch.tensor([title_len, text_len, score, comments], dtype=torch.float32)
        
        # æ ‡ç­¾
        label = torch.tensor(row. get('label', 0), dtype=torch.long)
        
        return text_features, label

# ==================== ç¬¬3éƒ¨åˆ†ï¼šæ¨¡å‹æ¶æ„ ====================
class MultimodalFakeNewsDetector(nn.Module):
    """å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹å™¨"""
    
    def __init__(self, text_feature_dim=4, hidden_dim=128):
        """
        å‚æ•°:
        - text_feature_dim: æ–‡æœ¬ç‰¹å¾ç»´åº¦
        - hidden_dim: éšè—å±‚ç»´åº¦
        """
        super(MultimodalFakeNewsDetector, self).__init__()
        
        print("\nğŸ—ï¸  æ„å»ºæ¨¡å‹æ¶æ„...")
        
        # æ–‡æœ¬å¤„ç†åˆ†æ”¯
        self.text_branch = nn.Sequential(
            nn.Linear(text_feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, 64),
            nn.ReLU()
        )
        
        # å›¾ç‰‡ç‰¹å¾æ¨¡æ‹Ÿåˆ†æ”¯ï¼ˆåœ¨çœŸå®é¡¹ç›®ä¸­ä¼šç”¨ResNet50ï¼‰
        self.image_branch = nn.Sequential(
            nn.Linear(10, hidden_dim),  # å‡è®¾å›¾ç‰‡ç‰¹å¾æ˜¯10ç»´
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, 64),
            nn.ReLU()
        )
        
        # èåˆå±‚
        self.fusion = nn. Sequential(
            nn.Linear(128, 64),  # 64+64=128
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(64, 32),
            nn.ReLU()
        )
        
        # åˆ†ç±»å±‚
        self.classifier = nn. Sequential(
            nn.Linear(32, 16),
            nn.ReLU(),
            nn.Linear(16, 2)  # äºŒåˆ†ç±»ï¼šçœŸ/å‡
        )
        
        print("âœ“ æ¨¡å‹æ„å»ºå®Œæˆ")
        print(f"  å‚æ•°æ•°é‡: {sum(p.numel() for p in self.parameters()):,}")
    
    def forward(self, text_features, image_features=None):
        """
        å‰å‘ä¼ æ’­
        """
        # æ–‡æœ¬å¤„ç†
        text_out = self.text_branch(text_features)
        
        # å›¾ç‰‡å¤„ç†ï¼ˆå¦‚æœæ²¡æœ‰å›¾ç‰‡ï¼Œç”¨éšæœºå‘é‡ï¼‰
        if image_features is None:
            image_features = torch.randn(text_features.size(0), 10, device=text_features.device)
        
        image_out = self.image_branch(image_features)
        
        # èåˆ
        combined = torch.cat([text_out, image_out], dim=1)
        fused = self.fusion(combined)
        
        # åˆ†ç±»
        output = self.classifier(fused)
        
        return output

# ==================== ç¬¬4éƒ¨åˆ†ï¼šè®­ç»ƒå™¨ ====================
class ModelTrainer:
    """æ¨¡å‹è®­ç»ƒå™¨"""
    
    def __init__(self, model, device, lr=1e-3):
        self.model = model.to(device)
        self.device = device
        
        self.criterion = nn.CrossEntropyLoss()
        self.optimizer = optim.Adam(model.parameters(), lr=lr)
        
        print(f"\nâš™ï¸  è®­ç»ƒé…ç½®:")
        print(f"  ä¼˜åŒ–å™¨: Adam")
        print(f"  å­¦ä¹ ç‡: {lr}")
        print(f"  æŸå¤±å‡½æ•°: CrossEntropyLoss")
    
    def train_epoch(self, dataloader):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for batch_idx, (text_features, labels) in enumerate(dataloader):
            text_features = text_features.to(self. device)
            labels = labels. to(self.device)
            
            # å‰å‘ä¼ æ’­
            outputs = self.model(text_features)
            loss = self.criterion(outputs, labels)
            
            # åå‘ä¼ æ’­
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            # ç»Ÿè®¡
            total_loss += loss.item()
            _, predicted = torch.max(outputs. data, 1)
            correct += (predicted == labels).sum().item()
            total += labels. size(0)
        
        accuracy = correct / total * 100
        avg_loss = total_loss / len(dataloader)
        
        return avg_loss, accuracy
    
    def evaluate(self, dataloader):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for text_features, labels in dataloader:
                text_features = text_features. to(self.device)
                labels = labels.to(self. device)
                
                outputs = self.model(text_features)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
        
        accuracy = correct / total * 100
        avg_loss = total_loss / len(dataloader)
        
        return avg_loss, accuracy

# ==================== ç¬¬5éƒ¨åˆ†ï¼šä¸»ç¨‹åº ====================
def main():
    print("\n" + "="*70)
    print("ğŸš€ å¼€å§‹è®­ç»ƒå¤šæ¨¡æ€æ¨¡å‹")
    print("="*70)
    
    # 1. åŠ è½½æ•°æ®
    print("\n[1/5] åŠ è½½è®­ç»ƒæ•°æ®...")
    train_dataset = TextImageDataset('model_data/train_set.csv', max_samples=100)
    val_dataset = TextImageDataset('model_data/val_set.csv', max_samples=30)
    
    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)
    
    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ")
    print(f"  è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_loader)}")
    print(f"  éªŒè¯æ‰¹æ¬¡æ•°: {len(val_loader)}")
    
    # 2. åˆ›å»ºæ¨¡å‹
    print("\n[2/5] åˆ›å»ºæ¨¡å‹...")
    model = MultimodalFakeNewsDetector(text_feature_dim=4, hidden_dim=128)
    
    # 3. åˆ›å»ºè®­ç»ƒå™¨
    print("\n[3/5] åˆå§‹åŒ–è®­ç»ƒå™¨...")
    trainer = ModelTrainer(model, device, lr=1e-3)
    
    # 4. è®­ç»ƒå¾ªç¯
    print("\n[4/5] å¼€å§‹è®­ç»ƒ...")
    print("="*70)
    
    num_epochs = 5
    best_val_loss = float('inf')
    history = {'train_loss': [], 'train_acc': [], 'val_loss':  [], 'val_acc': []}
    
    for epoch in range(num_epochs):
        # è®­ç»ƒ
        train_loss, train_acc = trainer.train_epoch(train_loader)
        
        # éªŒè¯
        val_loss, val_acc = trainer.evaluate(val_loader)
        
        # è®°å½•
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        
        # æ˜¾ç¤º
        print(f"\nEpoch [{epoch+1}/{num_epochs}]")
        print(f"  è®­ç»ƒ:  Loss={train_loss:.4f} Acc={train_acc:.2f}%")
        print(f"  éªŒè¯: Loss={val_loss:.4f} Acc={val_acc:.2f}%")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            torch.save(model.state_dict(), 'model_data/best_model.pth')
            print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹")
    
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print("="*70)
    
    # 5. ä¿å­˜ç»“æœ
    print("\n[5/5] ä¿å­˜æ¨¡å‹å’Œå†å²...")
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), 'model_data/final_model.pth')
    
    # ä¿å­˜è®­ç»ƒå†å²
    with open('model_data/training_history.json', 'w') as f:
        # è½¬æ¢ä¸ºåˆ—è¡¨ä¾¿äºJSONåºåˆ—åŒ–
        history_serializable = {k: [float(v) for v in vals] for k, vals in history.items()}
        json.dump(history_serializable, f, indent=2)
    
    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜:  model_data/best_model.pth")
    print(f"âœ“ è®­ç»ƒå†å²å·²ä¿å­˜:  model_data/training_history.json")
    
    # æ˜¾ç¤ºæœ€ç»ˆç»“æœ
    print("\n" + "="*70)
    print("ğŸ“Š æœ€ç»ˆç»“æœ")
    print("="*70)
    print(f"\nè®­ç»ƒé›†:")
    print(f"  æœ€ç»ˆLoss: {history['train_loss'][-1]:.4f}")
    print(f"  æœ€ç»ˆç²¾åº¦: {history['train_acc'][-1]:.2f}%")
    
    print(f"\néªŒè¯é›†:")
    print(f"  æœ€ç»ˆLoss: {history['val_loss'][-1]:.4f}")
    print(f"  æœ€ç»ˆç²¾åº¦: {history['val_acc'][-1]:.2f}%")
    
    print(f"\næœ€ä½³éªŒè¯ç²¾åº¦: {max(history['val_acc']):.2f}%")
    
    print("\n" + "="*70)
    print("ğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆ!")
    print("="*70)

if __name__ == "__main__": 
    main()