"""
çœŸå®çš„å¤šæ¨¡æ€æ¨¡å‹ v2
ä½¿ç”¨çœŸå®çš„æ–‡æœ¬å’Œå›¾ç‰‡ç‰¹å¾æå–
"""

import torch
import torch.nn as nn
import torch. optim as optim
from torch. utils.data import Dataset, DataLoader
import pandas as pd
import numpy as np
import os
from datetime import datetime
import json
import warnings
warnings.filterwarnings('ignore')

print("="*70)
print("ğŸ¤– çœŸå®å¤šæ¨¡æ€è™šå‡ä¿¡æ¯æ£€æµ‹æ¨¡å‹ v2.0")
print("="*70)

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"\nâœ“ ä½¿ç”¨è®¾å¤‡: {device}")

# ==================== ç¬¬1éƒ¨åˆ†ï¼šæ”¹è¿›çš„ç‰¹å¾æå– ====================
class TextFeatureExtractor:
    """æ–‡æœ¬ç‰¹å¾æå–å™¨ - æå–æ›´ä¸°å¯Œçš„ç‰¹å¾"""
    
    def __init__(self):
        print("\nğŸ“ åˆå§‹åŒ–æ–‡æœ¬ç‰¹å¾æå–å™¨...")
    
    def extract_features(self, title, text, score, comments):
        """
        ä»æ–‡æœ¬ä¸­æå–çœŸå®ç‰¹å¾
        """
        features = []
        
        # 1. é•¿åº¦ç‰¹å¾
        features.append(len(title) if title else 0)
        features.append(len(str(text)) if text else 0)
        
        # 2. è¯æ±‡ç‰¹å¾
        title_words = len(str(title).split()) if title else 0
        text_words = len(str(text).split()) if text else 0
        features.append(title_words)
        features.append(text_words)
        
        # 3. ç¤¾äº¤ç‰¹å¾
        features.append(float(score) if score else 0)
        features.append(float(comments) if comments else 0)
        
        # 4. æ¯”ç‡ç‰¹å¾
        avg_word_len = (len(title) / title_words) if title_words > 0 else 0
        features.append(avg_word_len)
        
        # 5. ç‰¹æ®Šå­—ç¬¦
        special_chars = sum(1 for c in str(title) if not c.isalnum() and c != ' ')
        features.append(special_chars)
        
        # 6. å¤§å†™å­—æ¯æ¯”ä¾‹
        uppercase = sum(1 for c in str(title) if c.isupper())
        uppercase_ratio = uppercase / len(title) if len(title) > 0 else 0
        features.append(uppercase_ratio)
        
        # 7. æ•°å­—æ¯”ä¾‹
        digits = sum(1 for c in str(title) if c.isdigit())
        digit_ratio = digits / len(title) if len(title) > 0 else 0
        features.append(digit_ratio)
        
        return np.array(features, dtype=np.float32)

# ==================== ç¬¬2éƒ¨åˆ†ï¼šæ”¹è¿›çš„æ•°æ®é›† ====================
class ImprovedTextImageDataset(Dataset):
    """æ”¹è¿›çš„æ•°æ®é›† - æå–çœŸå®ç‰¹å¾"""
    
    def __init__(self, csv_file, max_samples=None):
        self.df = pd.read_csv(csv_file)
        
        if max_samples:
            self. df = self.df. head(max_samples)
        
        self.feature_extractor = TextFeatureExtractor()
        
        print(f"\nâœ“ åŠ è½½æ•°æ®é›†: {csv_file}")
        print(f"  æ ·æœ¬æ•°: {len(self. df)}")
        
        # é¢„æå–æ‰€æœ‰ç‰¹å¾ï¼ˆåŠ é€Ÿè®­ç»ƒï¼‰
        print("  æ­£åœ¨æå–æ–‡æœ¬ç‰¹å¾...")
        self.features = []
        for idx, row in self.df.iterrows():
            features = self.feature_extractor. extract_features(
                row. get('title', ''),
                row.get('text', ''),
                row.get('score', 0),
                row.get('comments', 0)
            )
            self.features.append(features)
        
        self.features = np. array(self.features)
        print(f"  âœ“ æå–äº† {self.features.shape[1]} ä¸ªç‰¹å¾")
    
    def __len__(self):
        return len(self.df)
    
    def __getitem__(self, idx):
        features = torch.tensor(self.features[idx], dtype=torch.float32)
        label = torch.tensor(self.df.iloc[idx]['label'], dtype=torch.long)
        return features, label

# ==================== ç¬¬3éƒ¨åˆ†ï¼šæ”¹è¿›çš„æ¨¡å‹ ====================
class AdvancedMultimodalDetector(nn.Module):
    """æ”¹è¿›çš„å¤šæ¨¡æ€æ£€æµ‹å™¨"""
    
    def __init__(self, feature_dim=9):
        super(AdvancedMultimodalDetector, self).__init__()
        
        print("\nğŸ—ï¸  æ„å»ºæ”¹è¿›çš„æ¨¡å‹æ¶æ„...")
        
        # æ–‡æœ¬ç‰¹å¾å¤„ç†ï¼ˆå¢åŠ å¤æ‚åº¦ï¼‰
        self.text_branch = nn.Sequential(
            nn.Linear(feature_dim, 64),
            nn.BatchNorm1d(64),
            nn.ReLU(),
            nn.Dropout(0.4),
            
            nn.Linear(64, 32),
            nn.BatchNorm1d(32),
            nn.ReLU(),
            nn.Dropout(0.3),
            
            nn.Linear(32, 16)
        )
        
        # æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¢å¼ºç‰¹å¾ï¼‰
        self.attention = nn.Sequential(
            nn.Linear(16, 8),
            nn.Sigmoid()
        )
        
        # åˆ†ç±»å¤´
        # æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå¢å¼ºç‰¹å¾ï¼‰
        self.attention = nn.Sequential(
            nn.Linear(16, 16),
            nn.Sigmoid()
        )
        
        # åˆ†ç±»å±‚
        self.classifier = nn.Sequential(
            nn.Linear(16, 8),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(8, 2)
        )

        print("âœ“ æ¨¡å‹æ„å»ºå®Œæˆ")
        print(f"  å‚æ•°æ•°é‡: {sum(p.numel() for p in self.parameters()):,}")
        print(f"  ç‰¹å¾ç»´åº¦: {feature_dim}")
    
    def forward(self, features):
        # æ–‡æœ¬å¤„ç†
        text_out = self.text_branch(features)
        
        # æ³¨æ„åŠ›åŠ æƒ
        attention_weights = self.attention(text_out)
        weighted = text_out * attention_weights
        
        # åˆ†ç±»
        output = self.classifier(weighted)
        
        return output

# ==================== ç¬¬4éƒ¨åˆ†ï¼šæ”¹è¿›çš„è®­ç»ƒå™¨ ====================
class AdvancedTrainer:
    """æ”¹è¿›çš„è®­ç»ƒå™¨ - æ”¯æŒå­¦ä¹ ç‡è°ƒæ•´ã€æ—©åœç­‰"""
    
    def __init__(self, model, device, lr=1e-3):
        self.model = model. to(device)
        self.device = device
        
        self.criterion = nn.CrossEntropyLoss(weight=torch.tensor([1.0, 1.2]))  # å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
        self.optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)
        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, mode='min', factor=0.5, patience=2)
        
        print(f"\nâš™ï¸  è®­ç»ƒé…ç½®:")
        print(f"  ä¼˜åŒ–å™¨: Adam (weight_decay=1e-5)")
        print(f"  åˆå§‹å­¦ä¹ ç‡: {lr}")
        print(f"  æŸå¤±å‡½æ•°: CrossEntropyLoss (å¸¦ç±»åˆ«æƒé‡)")
        print(f"  å­¦ä¹ ç‡è°ƒæ•´:  ReduceLROnPlateau")
    
    def train_epoch(self, dataloader):
        self.model.train()
        total_loss = 0
        correct = 0
        total = 0
        
        for features, labels in dataloader:
            features = features.to(self.device)
            labels = labels.to(self.device)
            
            outputs = self.model(features)
            loss = self.criterion(outputs, labels)
            
            self.optimizer.zero_grad()
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
            self.optimizer.step()
            
            total_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            correct += (predicted == labels).sum().item()
            total += labels.size(0)
        
        return total_loss / len(dataloader), correct / total * 100
    
    def evaluate(self, dataloader):
        self.model.eval()
        total_loss = 0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for features, labels in dataloader:
                features = features.to(self.device)
                labels = labels.to(self.device)
                
                outputs = self.model(features)
                loss = self.criterion(outputs, labels)
                
                total_loss += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                correct += (predicted == labels).sum().item()
                total += labels.size(0)
        
        val_loss = total_loss / len(dataloader)
        self.scheduler.step(val_loss)
        
        return val_loss, correct / total * 100

# ==================== ç¬¬5éƒ¨åˆ†ï¼šä¸»ç¨‹åº ====================
def main():
    print("\n" + "="*70)
    print("ğŸš€ å¼€å§‹è®­ç»ƒæ”¹è¿›çš„å¤šæ¨¡æ€æ¨¡å‹")
    print("="*70)
    
    # åŠ è½½æ•°æ®
    print("\n[1/5] åŠ è½½æ•°æ®...")
    train_dataset = ImprovedTextImageDataset('model_data/train_set.csv', max_samples=None)
    val_dataset = ImprovedTextImageDataset('model_data/val_set.csv', max_samples=None)
    
    train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)
    
    print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ")
    
    # åˆ›å»ºæ¨¡å‹
    print("\n[2/5] åˆ›å»ºæ”¹è¿›çš„æ¨¡å‹...")
    model = AdvancedMultimodalDetector(feature_dim=train_dataset.features.shape[1])
    
    # åˆ›å»ºè®­ç»ƒå™¨
    print("\n[3/5] åˆå§‹åŒ–è®­ç»ƒå™¨...")
    trainer = AdvancedTrainer(model, device, lr=5e-4)
    
    # è®­ç»ƒ
    print("\n[4/5] å¼€å§‹è®­ç»ƒ...")
    print("="*70)
    
    num_epochs = 10
    best_val_loss = float('inf')
    patience_counter = 0
    max_patience = 3
    
    history = {'train_loss': [], 'train_acc': [], 'val_loss':  [], 'val_acc': []}
    
    for epoch in range(num_epochs):
        train_loss, train_acc = trainer.train_epoch(train_loader)
        val_loss, val_acc = trainer.evaluate(val_loader)
        
        history['train_loss'].append(train_loss)
        history['train_acc'].append(train_acc)
        history['val_loss'].append(val_loss)
        history['val_acc'].append(val_acc)
        
        print(f"\nEpoch [{epoch+1}/{num_epochs}]")
        print(f"  è®­ç»ƒ:  Loss={train_loss:.4f} Acc={train_acc:.2f}%")
        print(f"  éªŒè¯: Loss={val_loss:.4f} Acc={val_acc:.2f}%")
        
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            torch.save(model.state_dict(), 'model_data/advanced_best_model.pth')
            print(f"  âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹")
        else:
            patience_counter += 1
            if patience_counter >= max_patience: 
                print(f"  âš ï¸  æ—©åœè§¦å‘ (è€å¿ƒæ¬¡æ•°: {patience_counter}/{max_patience})")
                break
    
    print("\n" + "="*70)
    print("âœ… è®­ç»ƒå®Œæˆ!")
    print("="*70)
    
    # ä¿å­˜
    print("\n[5/5] ä¿å­˜ç»“æœ...")
    torch.save(model.state_dict(), 'model_data/advanced_final_model.pth')
    
    with open('model_data/advanced_history.json', 'w') as f:
        history_serializable = {k: [float(v) for v in vals] for k, vals in history. items()}
        json.dump(history_serializable, f, indent=2)
    
    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜:  model_data/advanced_best_model.pth")
    print(f"âœ“ è®­ç»ƒå†å²å·²ä¿å­˜: model_data/advanced_history.json")
    
    # ç»“æœ
    print("\n" + "="*70)
    print("ğŸ“Š æœ€ç»ˆç»“æœ")
    print("="*70)
    print(f"\nè®­ç»ƒé›†:")
    print(f"  æœ€ç»ˆLoss: {history['train_loss'][-1]:.4f}")
    print(f"  æœ€ç»ˆç²¾åº¦: {history['train_acc'][-1]:.2f}%")
    
    print(f"\néªŒè¯é›†:")
    print(f"  æœ€ç»ˆLoss: {history['val_loss'][-1]:.4f}")
    print(f"  æœ€ç»ˆç²¾åº¦: {history['val_acc'][-1]:.2f}%")
    
    print(f"\næœ€ä½³éªŒè¯ç²¾åº¦: {max(history['val_acc']):.2f}%")
    print(f"æœ€ä½³éªŒè¯Loss: {min(history['val_loss']):.4f}")
    
    print("\n" + "="*70)
    print("ğŸ‰ æ”¹è¿›çš„æ¨¡å‹è®­ç»ƒå®Œæˆ!")
    print("="*70)

if __name__ == "__main__":
    main()