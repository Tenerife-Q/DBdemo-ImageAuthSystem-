"""
æ”¹è¿›ç‰ˆçˆ¬è™« - è§£å†³Wikipediaå›¾ç‰‡ä¸‹è½½é—®é¢˜
ä½¿ç”¨æ›´æ™ºèƒ½çš„å›¾ç‰‡è¯†åˆ«å’Œå¤„ç†
"""

import requests
from bs4 import BeautifulSoup
import os
from urllib.parse import urljoin
from PIL import Image
from io import BytesIO
import time
from tqdm import tqdm

class ImprovedScraper:
    """æ”¹è¿›çš„çˆ¬è™«"""
    
    HEADERS = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
    }
    
    def __init__(self, output_dir='downloaded_data_v2'):
        self.output_dir = output_dir
        os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)
        print(f"âœ“ åˆå§‹åŒ–å®Œæˆ:  {output_dir}/")
    
    def scrape(self, url, max_images=15):
        """
        æ”¹è¿›çš„çˆ¬è™«é€»è¾‘
        """
        print("\n" + "="*70)
        print(f"ğŸ” å¼€å§‹çˆ¬å–: {url}")
        print("="*70)
        
        try:
            # 1. ä¸‹è½½ç½‘é¡µ
            print("\n[1/5] ä¸‹è½½ç½‘é¡µ...")
            response = requests.get(url, headers=self.HEADERS, timeout=10)
            response. encoding = 'utf-8'
            print(f"âœ… æˆåŠŸ (å¤§å°: {len(response.text)/1024:.1f} KB)")
            
            # 2. è§£æHTML
            print("\n[2/5] è§£æHTML...")
            soup = BeautifulSoup(response.text, 'html.parser')
            print("âœ… è§£æå®Œæˆ")
            
            # 3. æå–æ‰€æœ‰å›¾ç‰‡
            print("\n[3/5] æå–å›¾ç‰‡é“¾æ¥...")
            all_img_tags = soup.find_all('img')
            print(f"âœ… æ‰¾åˆ° {len(all_img_tags)} ä¸ªå›¾ç‰‡æ ‡ç­¾")
            
            # 4. è¿‡æ»¤æœ‰ç”¨çš„å›¾ç‰‡
            print("\n[4/5] è¿‡æ»¤å›¾ç‰‡...")
            useful_images = []
            
            for img in all_img_tags:
                src = img.get('src') or img.get('data-src')
                if not src:
                    continue
                
                # è½¬æ¢ç›¸å¯¹URLä¸ºç»å¯¹URL
                if src.startswith('//'):
                    src = 'https:' + src
                elif src.startswith('/'):
                    src = urljoin(url, src)
                
                # è¿‡æ»¤æ‰å¤ªå°çš„å›¾ç‰‡ï¼ˆiconç­‰ï¼‰
                width = img.get('width', '0')
                height = img. get('height', '0')
                
                try:
                    if int(width or 0) > 150 and int(height or 0) > 150:
                        useful_images.append(src)
                except: 
                    # å¦‚æœæ²¡æœ‰width/heightå±æ€§ï¼Œä¹ŸåŠ å…¥ï¼ˆç¨ååˆ¤æ–­ï¼‰
                    useful_images.append(src)
            
            print(f"âœ… è¿‡æ»¤å:  {len(useful_images)} ä¸ªæœ‰ç”¨çš„å›¾ç‰‡")
            
            # 5. ä¸‹è½½å›¾ç‰‡
            print(f"\n[5/5] ä¸‹è½½å›¾ç‰‡ (æœ€å¤š {max_images} å¼ )...")
            
            downloaded = 0
            failed = 0
            skipped = 0
            
            # ç”¨è¿›åº¦æ¡
            for idx, img_url in enumerate(useful_images[: max_images]):
                try:
                    # ä¸‹è½½
                    img_response = requests.get(img_url, timeout=5, headers=self.HEADERS)
                    
                    if img_response.status_code != 200:
                        failed += 1
                        continue
                    
                    # æ‰“å¼€å›¾ç‰‡
                    try:
                        img = Image. open(BytesIO(img_response.content))
                    except: 
                        # å¦‚æœPILæ— æ³•è¯†åˆ«ï¼Œå°è¯•ç”¨å…¶ä»–æ–¹å¼
                        failed += 1
                        continue
                    
                    # æ£€æŸ¥å›¾ç‰‡å¤§å°ï¼ˆè¿‡æ»¤å¤ªå°çš„ï¼‰
                    width, height = img.size
                    if width < 150 or height < 150:
                        skipped += 1
                        continue
                    
                    # è½¬æ¢ä¸ºRGB
                    if img.mode != 'RGB': 
                        img = img.convert('RGB')
                    
                    # ä¿å­˜
                    filename = f"image_{downloaded: 03d}. jpg"
                    filepath = os.path.join(self.output_dir, 'images', filename)
                    img.save(filepath, quality=85, optimize=True)
                    
                    downloaded += 1
                    print(f"   âœ“ [{downloaded}] {width}x{height} - {filename}")
                    
                except Exception as e:
                    failed += 1
            
            # æå–æ–‡æœ¬
            print(f"\n[æå–æ–‡æœ¬]...")
            paragraphs = soup.find_all('p')
            texts = []
            for p in paragraphs[: 15]: 
                text = p.get_text().strip()
                if len(text) > 50:  # åªè¦>50å­—çš„æ®µè½
                    texts.append(text)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata_path = os.path.join(self.output_dir, 'metadata.txt')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                f.write("="*70 + "\n")
                f.write("çˆ¬è™«æ•°æ®æ±‡æ€»\n")
                f.write("="*70 + "\n\n")
                f.write(f"æ•°æ®æº: {url}\n")
                f.write(f"çˆ¬å–æ—¶é—´: {time. strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                f.write(f"å›¾ç‰‡ç»Ÿè®¡:\n")
                f.write(f"  æˆåŠŸ: {downloaded} å¼ \n")
                f.write(f"  å¤±è´¥: {failed} å¼ \n")
                f.write(f"  è·³è¿‡: {skipped} å¼ \n\n")
                f.write("æå–çš„æ–‡æœ¬:\n")
                f.write("-"*70 + "\n")
                for i, text in enumerate(texts, 1):
                    f.write(f"\n{i}. {text[: 150]}...\n")
            
            # æ€»ç»“
            print("\n" + "="*70)
            print("âœ… çˆ¬è™«å®Œæˆ!")
            print("="*70)
            print(f"ğŸ“Š ç»Ÿè®¡:")
            print(f"   âœ“ æˆåŠŸä¸‹è½½: {downloaded} å¼ å›¾ç‰‡")
            print(f"   âœ— ä¸‹è½½å¤±è´¥: {failed} å¼ ")
            print(f"   âŠ˜ è·³è¿‡: {skipped} å¼ ")
            print(f"   ğŸ“ æ–‡æœ¬æ®µè½:  {len(texts)} æ®µ")
            print(f"\nğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}/")
            print(f"   â”œâ”€â”€ images/ ({downloaded} å¼ å›¾ç‰‡)")
            print(f"   â””â”€â”€ metadata.txt")
            print("="*70)
            
            return {
                'downloaded': downloaded,
                'failed': failed,
                'skipped': skipped,
                'texts': len(texts)
            }
            
        except Exception as e: 
            print(f"\nâŒ é”™è¯¯: {e}")
            return None

# ==================== è¿è¡Œ ====================
if __name__ == "__main__":
    scraper = ImprovedScraper('downloaded_data_v2')
    
    # è¯•è¯•çˆ¬è™šå‡æ–°é—»ç›¸å…³çš„Wikipediaé¡µé¢
    scraper.scrape(
        url='https://en.wikipedia.org/wiki/Fake_news',
        max_images=15
    )