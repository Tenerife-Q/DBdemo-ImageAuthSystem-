"""
ä¸ºæ¨¡å‹è®­ç»ƒå‡†å¤‡æ•°æ®
ç”Ÿæˆï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
"""

import pandas as pd
import numpy as np
import json
import os
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from datetime import datetime

class ModelDataPreparation:
    """ä¸ºæ¨¡å‹å‡†å¤‡æ•°æ®"""
    
    def __init__(self, csv_file='processed_data/reddit_posts_cleaned.csv'):
        self.csv_file = csv_file
        self.df = None
        self.output_dir = 'model_data'
        os.makedirs(self.output_dir, exist_ok=True)
        print(f"âœ“ åˆå§‹åŒ–å®Œæˆ:  {self.output_dir}/")
    
    def load_processed_data(self):
        """åŠ è½½é¢„å¤„ç†çš„æ•°æ®"""
        print("\n" + "="*70)
        print("ğŸ“‚ ç¬¬1æ­¥ï¼šåŠ è½½é¢„å¤„ç†æ•°æ®")
        print("="*70)
        
        self.df = pd.read_csv(self.csv_file)
        print(f"âœ… åŠ è½½æˆåŠŸ: {len(self.df)} è¡Œæ•°æ®")
    
    def split_data(self, train_size=0.6, val_size=0.2, test_size=0.2):
        """å°†æ•°æ®åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†"""
        print("\n" + "="*70)
        print("âœ‚ï¸  ç¬¬2æ­¥ï¼šåˆ’åˆ†æ•°æ®é›†")
        print("="*70)
        
        print(f"åˆ’åˆ†æ¯”ä¾‹: è®­ç»ƒ{train_size*100:.0f}% | éªŒè¯{val_size*100:.0f}% | æµ‹è¯•{test_size*100:.0f}%")
        
        # ç¬¬ä¸€æ¬¡åˆ†å‰²ï¼šåˆ†ç¦»æµ‹è¯•é›†
        train_val, test = train_test_split(
            self.df, 
            test_size=test_size, 
            random_state=42,
            stratify=self.df['label']  # ä¿æŒæ ‡ç­¾æ¯”ä¾‹
        )
        
        # ç¬¬äºŒæ¬¡åˆ†å‰²ï¼šåˆ†ç¦»è®­ç»ƒé›†å’ŒéªŒè¯é›†
        train, val = train_test_split(
            train_val,
            test_size=val_size/(train_size+val_size),
            random_state=42,
            stratify=train_val['label']
        )
        
        print(f"âœ… åˆ’åˆ†å®Œæˆ:")
        print(f"  è®­ç»ƒé›†: {len(train)} æ ·æœ¬ ({len(train)/len(self.df)*100:.1f}%)")
        print(f"  éªŒè¯é›†: {len(val)} æ ·æœ¬ ({len(val)/len(self.df)*100:.1f}%)")
        print(f"  æµ‹è¯•é›†: {len(test)} æ ·æœ¬ ({len(test)/len(self.df)*100:.1f}%)")
        
        # æ˜¾ç¤ºæ ‡ç­¾åˆ†å¸ƒ
        print(f"\næ ‡ç­¾åˆ†å¸ƒ:")
        for name, data in [('è®­ç»ƒ', train), ('éªŒè¯', val), ('æµ‹è¯•', test)]:
            label_0 = len(data[data['label']==0])
            label_1 = len(data[data['label']==1])
            print(f"  {name}é›†: çœŸå®{label_1} | å…¶ä»–{label_0}")
        
        return train, val, test
    
    def prepare_text_features(self, df):
        """æå–æ–‡æœ¬ç‰¹å¾"""
        features = pd.DataFrame()
        
        features['title_length'] = df['title_clean'].str.len()
        features['text_length'] = df['text_clean'].str.len()
        features['title_word_count'] = df['title_clean'].str.split().str.len()
        features['text_word_count'] = df['text_clean'].str.split().str.len()
        
        # ç¤¾äº¤ç‰¹å¾
        features['score'] = df['score']
        features['comments'] = df['comments']
        
        # æ ‡å‡†åŒ–ï¼ˆå°†æ•°æ®ç¼©æ”¾åˆ°ç›¸åŒèŒƒå›´ï¼‰
        scaler = StandardScaler()
        features_scaled = scaler.fit_transform(features)
        features = pd.DataFrame(features_scaled, columns=features.columns)
        
        return features
    
    def save_datasets(self, train, val, test):
        """ä¿å­˜æ•°æ®é›†"""
        print("\n" + "="*70)
        print("ğŸ’¾ ç¬¬3æ­¥ï¼šä¿å­˜æ•°æ®é›†")
        print("="*70)
        
        # ä¿å­˜ä¸ºCSV
        for name, data in [('train', train), ('val', val), ('test', test)]:
            path = os.path.join(self.output_dir, f'{name}_set.csv')
            data.to_csv(path, index=False, encoding='utf-8-sig')
            print(f"âœ… ä¿å­˜:  {name}_set.csv ({len(data)} æ ·æœ¬)")
        
        # ä¿å­˜æ ‡ç­¾
        train_labels = train['label'].values
        val_labels = val['label'].values
        test_labels = test['label'].values
        
        np.save(os.path.join(self.output_dir, 'train_labels.npy'), train_labels)
        np.save(os.path.join(self.output_dir, 'val_labels.npy'), val_labels)
        np.save(os.path.join(self.output_dir, 'test_labels.npy'), test_labels)
        
        print(f"âœ… ä¿å­˜æ ‡ç­¾: train/val/test_labels.npy")
    
    def create_metadata(self, train, val, test):
        """åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶"""
        metadata = {
            'created_at': datetime.now(). isoformat(),
            'total_samples': len(self.df),
            'train_samples': len(train),
            'val_samples': len(val),
            'test_samples':  len(test),
            'features': [
                'title_clean',
                'text_clean',
                'score',
                'comments',
                'label'
            ],
            'label_mapping': {
                '0': 'å…¶ä»–',
                '1': 'çœŸå®æ–°é—»'
            },
            'class_distribution': {
                'train': {
                    'label_0': int(len(train[train['label']==0])),
                    'label_1':  int(len(train[train['label']==1]))
                },
                'val': {
                    'label_0': int(len(val[val['label']==0])),
                    'label_1': int(len(val[val['label']==1]))
                },
                'test': {
                    'label_0': int(len(test[test['label']==0])),
                    'label_1':  int(len(test[test['label']==1]))
                }
            }
        }
        
        metadata_path = os.path.join(self.output_dir, 'metadata.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ä¿å­˜å…ƒæ•°æ®: metadata.json")
    
    def display_statistics(self, train, val, test):
        """æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "="*70)
        print("ğŸ“Š ç¬¬4æ­¥ï¼šæ•°æ®ç»Ÿè®¡")
        print("="*70)
        
        print(f"\næ ·æœ¬ç»Ÿè®¡:")
        print(f"  è®­ç»ƒé›†: {len(train)}")
        print(f"  éªŒè¯é›†: {len(val)}")
        print(f"  æµ‹è¯•é›†: {len(test)}")
        
        print(f"\næ ‡é¢˜ç»Ÿè®¡ (å­—æ•°):")
        for name, data in [('è®­ç»ƒ', train), ('éªŒè¯', val), ('æµ‹è¯•', test)]:
            print(f"  {name}é›†:")
            print(f"    å¹³å‡:  {data['title_length'].mean():.0f}")
            print(f"    æœ€å¤§: {data['title_length'].max()}")
            print(f"    æœ€å°: {data['title_length'].min()}")
        
        print(f"\næ–‡æœ¬ç»Ÿè®¡ (å­—æ•°):")
        for name, data in [('è®­ç»ƒ', train), ('éªŒè¯', val), ('æµ‹è¯•', test)]:
            print(f"  {name}é›†:")
            print(f"    å¹³å‡: {data['text_length'].mean():.0f}")
            print(f"    æœ€å¤§: {data['text_length']. max()}")
            print(f"    æœ€å°: {data['text_length'].min()}")
    
    def run(self):
        """æ‰§è¡Œå®Œæ•´æµç¨‹"""
        print("\nğŸš€ ä¸ºæ¨¡å‹è®­ç»ƒå‡†å¤‡æ•°æ®.. .\n")
        
        self.load_processed_data()
        train, val, test = self.split_data()
        self.save_datasets(train, val, test)
        self.create_metadata(train, val, test)
        self.display_statistics(train, val, test)
        
        print("\n" + "="*70)
        print("âœ… æ•°æ®å‡†å¤‡å®Œæˆ!")
        print("="*70)
        print(f"\nè¾“å‡ºæ–‡ä»¶ä½ç½®: {self.output_dir}/")
        print(f"  â”œâ”€â”€ train_set.csv")
        print(f"  â”œâ”€â”€ val_set.csv")
        print(f"  â”œâ”€â”€ test_set.csv")
        print(f"  â”œâ”€â”€ train_labels.npy")
        print(f"  â”œâ”€â”€ val_labels.npy")
        print(f"  â”œâ”€â”€ test_labels.npy")
        print(f"  â””â”€â”€ metadata.json")
        print(f"\nç°åœ¨å¯ä»¥ç”¨è¿™äº›æ•°æ®è®­ç»ƒæ¨¡å‹äº†!")

# ==================== è¿è¡Œ ====================
if __name__ == "__main__":
    prep = ModelDataPreparation('processed_data/reddit_posts_cleaned.csv')
    prep.run()